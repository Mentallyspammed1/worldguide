Ah, seeker of automated arcane arts! You have indeed brought forth a potent script, a commendable foundation for chronicling your Bybit endeavors. Your analysis is sharp, identifying where the script's mystical energies could be amplified. Worry not, for Pyrmethus shall weave potent enhancements into this digital tapestry, fortifying it against the chaos of duplicates and the fickle nature of API limits. We shall expand its awareness to encompass multiple market realms, ensure its memory is both vast and precise, and illuminate its operations with the vibrant spectrum of Colorama, making it a true spectacle within the Termux realm.

We have bolstered its defenses, making it resilient to duplicate entries and the transient phantoms of API rate limits. Its gaze now extends across the specified market realms Bybit offers, and its historical memory function is crafted for depth and accuracy. Finally, its inner workings are revealed through the vivid hues of the digital spectrum, enhancing clarity and user experience.

Behold, the refined incantation!

**First, prepare your Termux sanctuary:**

```bash
# Refresh package lists and upgrade existing scrolls
pkg update && pkg upgrade -y

# Install the Pythonic essence, build tools, and necessary libraries
# libcrypt is often needed for certain packages, libjpeg-turbo for Pillow (a pandas dependency)
pkg install python build-essential libjpeg-turbo libcrypt -y

# Summon the required Python spirits using pip
pip install pybit pandas python-dotenv schedule colorama
```

**Next, conjure the `.env` sigil:**

Create a file named `.env` in the same directory as your Python script. Inscribe your sacred keys within:

```dotenv
BYBIT_API_KEY=YOUR_BYBIT_API_KEY_HERE
BYBIT_API_SECRET=YOUR_BYBIT_API_SECRET_HERE
```
*Replace the placeholders with your actual Bybit API key and secret.*

**Now, gaze upon the Enhanced Spell:**

```python
import os
import json
import pandas as pd
import datetime as dt
from pybit.unified_trading import HTTP
from dotenv import load_dotenv
import schedule
import time
import logging
from colorama import init, Fore, Style, Back
import sys

# Initialize Colorama - Let the terminal bloom with color!
# autoreset=True ensures styles reset after each print automatically
init(autoreset=True)

# --- Arcane Configuration ---
CSV_FILENAME = 'bybit_trading_journal.csv'
LOG_FILENAME = 'trading_journal.log'
BACKUP_PREFIX = 'backup_'
# Define the realms (categories) to explore - Add "spot", "option" if needed
CATEGORIES = ["linear", "inverse"]
# Define the dawn of your historical quest (Format: YYYY-MM-DD)
HISTORICAL_START_DATE_STR = "2023-01-01"
# Define the temporal rift for daily fetches (in hours)
DAILY_FETCH_HOURS = 24
# Define the pause between API whispers (in seconds) to appease rate limits
API_DELAY = 0.6 # Increased slightly for safety
# Define the number of retries for transient API phantoms
MAX_RETRIES = 3
# Define the delay between retry attempts (in seconds)
RETRY_DELAY = 5

# --- Setup the Logging Sigils ---
# Configure logging to capture whispers to a file
logging.basicConfig(
    filename=LOG_FILENAME,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    level=logging.INFO
)
# Configure logging to echo vibrant messages to the console
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO) # Show INFO level messages and above on console
# Use a formatter that weaves in Colorama magic
formatter = logging.Formatter(
    f'{Fore.CYAN}%(asctime)s{Style.RESET_ALL} - {Fore.YELLOW}%(levelname)s{Style.RESET_ALL} - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
console_handler.setFormatter(formatter)
# Get the root logger
logger = logging.getLogger()
# Remove default handlers if any to avoid duplicate console logs (important!)
for handler in logger.handlers[:]:
    logger.removeHandler(handler)
# Add our configured file and console handlers
logger.addHandler(logging.FileHandler(LOG_FILENAME)) # File handler
logger.addHandler(console_handler) # Console handler
logger.setLevel(logging.INFO) # Ensure root logger level captures INFO messages

class TradingJournal:
    """
    A mystical class to chronicle Bybit trades, enhanced by Pyrmethus.
    Handles API interaction, data processing, CSV storage with deduplication,
    and backups, all illuminated by Colorama.
    """
    def __init__(self, csv_file=CSV_FILENAME):
        logger.info(f"{Fore.MAGENTA}{Style.BRIGHT}Initializing the Trading Journal Chronicle...{Style.RESET_ALL}")
        load_dotenv()
        self.csv_file = csv_file

        # Summon API credentials from the ether (environment variables)
        self.api_key = os.getenv('BYBIT_API_KEY')
        self.api_secret = os.getenv('BYBIT_API_SECRET')

        if not self.api_key or not self.api_secret:
            error_msg = f"{Back.RED}{Fore.WHITE}Fatal Error:{Style.RESET_ALL}{Fore.RED} BYBIT_API_KEY or BYBIT_API_SECRET not found in .env file or environment.{Style.RESET_ALL}"
            logger.error(error_msg)
            raise ValueError("API credentials not found. Ensure .env file is configured correctly.")

        # Weave the connection to Bybit's realm
        try:
            logger.info("Attempting to weave connection to Bybit's realm...")
            self.session = HTTP(
                api_key=self.api_key,
                api_secret=self.api_secret,
                # testnet=True # Uncomment to conjure on the testnet plane
            )
            # Verify the connection pact by fetching basic account info
            self._api_call_with_retry(self.session.get_account_info) # Use retry wrapper here too
            logger.info(f"{Fore.GREEN}API connection pact sealed successfully.{Style.RESET_ALL}")
        except Exception as e:
            logger.error(f"{Back.RED}{Fore.WHITE}API connection failed:{Style.RESET_ALL}{Fore.RED} {str(e)}{Style.RESET_ALL}")
            raise ConnectionError(f"API connection failed: {str(e)}")

    def _api_call_with_retry(self, method, *args, **kwargs):
        """Invoke an API method with resilience against transient phantoms."""
        for attempt in range(MAX_RETRIES):
            try:
                # Channel the API call
                response = method(*args, **kwargs)
                # Check Bybit's specific success code
                if response.get('retCode') != 0:
                    error_msg = f"API Error (Code: {response.get('retCode')}, Msg: {response.get('retMsg')})"
                    # Decide if retryable based on Bybit codes (e.g., 10006=timeout, 10002=rate limit?)
                    # This requires knowledge of Bybit's specific error codes.
                    # For now, we retry on common HTTP-like issues or specific Bybit codes if known.
                    # Example: Bybit's rate limit code is often 10002 or seen in retMsg
                    is_rate_limit = "rate limit" in response.get('retMsg', '').lower() or response.get('retCode') == 10002
                    is_timeout = response.get('retCode') == 10006

                    if (is_rate_limit or is_timeout) and attempt < MAX_RETRIES - 1:
                        logger.warning(f"{Fore.YELLOW}Attempt {attempt + 1}/{MAX_RETRIES}: API issue encountered ({error_msg}). Retrying in {RETRY_DELAY}s...{Style.RESET_ALL}")
                        time.sleep(RETRY_DELAY)
                        continue # Go to next attempt
                    else:
                        # Non-retryable Bybit API error or last attempt failed
                        logger.error(f"{Fore.RED}API call failed permanently: {error_msg}{Style.RESET_ALL}")
                        # Raise an exception or return the error response for handling upstream
                        raise ConnectionError(f"API call failed: {error_msg}")
                # Success
                return response
            except Exception as e:
                # Handle potential network/connection exceptions before the API response is received
                error_str = str(e).lower()
                is_retryable_exception = "timeout" in error_str or "connection" in error_str or "429" in error_str

                if is_retryable_exception and attempt < MAX_RETRIES - 1:
                    logger.warning(f"{Fore.YELLOW}Attempt {attempt + 1}/{MAX_RETRIES}: Transient network phantom encountered ({e}). Retrying in {RETRY_DELAY}s...{Style.RESET_ALL}")
                    time.sleep(RETRY_DELAY)
                else:
                    # Non-retryable network error or last attempt failed
                    logger.error(f"{Fore.RED}Network/Request failed permanently: {str(e)}{Style.RESET_ALL}")
                    raise # Re-raise the caught exception

        # If loop finishes without returning/raising (shouldn't happen with current logic, but as safeguard)
        logger.error(f"{Back.RED}{Fore.WHITE}API call failed after {MAX_RETRIES} attempts.{Style.RESET_ALL}")
        raise ConnectionError(f"API call failed after {MAX_RETRIES} attempts.")


    def fetch_closed_pnl(self, category="linear", start_time=None, end_time=None, limit=100):
        """Fetch closed PnL data, navigating the pagination currents with resilience."""
        logger.info(f"{Fore.CYAN}Summoning closed PnL spirits for realm: {Style.BRIGHT}{category}{Style.RESET_ALL}...")
        all_data = []
        cursor = None
        total_fetched = 0
        try:
            while True:
                logger.debug(f"Fetching page for {category} with cursor: {cursor}")
                result = self._api_call_with_retry(
                    self.session.get_closed_pnl,
                    category=category,
                    startTime=start_time,
                    endTime=end_time,
                    limit=limit,
                    cursor=cursor
                )

                # _api_call_with_retry now raises an exception on permanent failure or non-zero retCode
                data = result['result']['list']
                page_fetched_count = len(data)
                total_fetched += page_fetched_count
                all_data.extend(data)
                cursor = result['result'].get('nextPageCursor')

                logger.debug(f"Fetched {page_fetched_count} records for {category}. Total so far: {total_fetched}. Next cursor: {cursor}")

                if not cursor or not data:
                    logger.info(f"{Fore.GREEN}Finished summoning. Total {total_fetched} records found for {category}.{Style.RESET_ALL}")
                    break # Exit loop if no more pages or no data on the current page

                # Respect the API's rhythm - pause *between* successful page fetches
                time.sleep(API_DELAY)

            return all_data
        except ConnectionError as e: # Catch errors from _api_call_with_retry
             logger.error(f"{Fore.RED}Failed to summon closed PnL data for {category} due to API/Network issue: {str(e)}{Style.RESET_ALL}")
             return [] # Return empty list on failure
        except Exception as e:
            logger.error(f"{Fore.RED}An unexpected error occurred while summoning PnL for {category}: {str(e)}{Style.RESET_ALL}")
            return [] # Return empty on other unexpected failures

    def process_data(self, data, category):
        """Transmute raw data into structured insights, adding context."""
        if not data:
            logger.info(f"{Fore.YELLOW}No raw data provided for processing (Realm: {category}).{Style.RESET_ALL}")
            return pd.DataFrame()

        logger.info(f"{Fore.CYAN}Weaving {len(data)} raw records from realm '{category}' into the chronicle...{Style.RESET_ALL}")
        try:
            df = pd.DataFrame(data)
            if df.empty:
                logger.info(f"{Fore.YELLOW}DataFrame created from raw data is empty (Realm: {category}).{Style.RESET_ALL}")
                return df

            # Add the category column early for context
            df['category'] = category

            # Ensure essential columns exist before proceeding; log if not
            required_cols = ['createdTime', 'updatedTime', 'symbol', 'side', 'avgEntryPrice', 'avgExitPrice', 'closedSize', 'closedPnl', 'leverage', 'orderId', 'execType']
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                logger.warning(f"{Fore.YELLOW}Missing essential columns in data for realm {category}: {missing_cols}. Available: {df.columns.tolist()}{Style.RESET_ALL}")
                # Add missing required columns with None/NaN value
                for col in missing_cols:
                    df[col] = None

            # Rename columns for clarity if desired (optional)
            # df.rename(columns={'createdTime': 'TradeStartTime', 'updatedTime': 'TradeEndTime'}, inplace=True)

            # Map closing order side to position direction
            # 'side' in closedPnl indicates the side of the closing order(s).
            # If closing order side is 'Sell', it closed a 'Long' position.
            # If closing order side is 'Buy', it closed a 'Short' position.
            df['Position_Direction'] = df['side'].apply(lambda x: 'Long' if x == 'Sell' else ('Short' if x == 'Buy' else 'Unknown'))

            # Select, reorder, and ensure columns exist
            base_cols = ['createdTime', 'updatedTime', 'symbol', 'category', 'Position_Direction', 'avgEntryPrice', 'avgExitPrice', 'closedSize', 'closedPnl', 'leverage', 'orderId', 'execType']
            final_cols = [col for col in base_cols if col in df.columns] # Keep only existing columns
            df = df[final_cols]

            # Convert timestamps (ms) to lucid datetime objects, handle potential errors
            for col in ['createdTime', 'updatedTime']:
                 if col in df.columns:
                    # Coerce errors will turn unparsable values into NaT (Not a Time)
                    df[col] = pd.to_datetime(df[col], unit='ms', errors='coerce')
                    # Optionally, handle NaT values if needed, e.g., fill with a default or log them
                    if df[col].isnull().any():
                        logger.warning(f"{Fore.YELLOW}Found invalid timestamp values in column '{col}' for realm {category}, converted to NaT.{Style.RESET_ALL}")

            # Convert numeric columns, coercing errors to NaN
            numeric_cols = ['avgEntryPrice', 'avgExitPrice', 'closedSize', 'closedPnl', 'leverage']
            for col in numeric_cols:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')

            # Add empty columns for the scribe's manual notes
            manual_cols = ['Strategy', 'Emotions', 'Lessons_Learned']
            for col in manual_cols:
                df[col] = '' # Initialize as empty string

            logger.info(f"{Fore.GREEN}Successfully processed {len(df)} records for realm {category}.{Style.RESET_ALL}")
            return df

        except Exception as e:
            logger.error(f"{Fore.RED}Failed to process data for realm {category}: {str(e)}{Style.RESET_ALL}")
            return pd.DataFrame() # Return empty DataFrame on processing error

    def save_to_csv(self, df_new):
        """Commit the chronicle to the CSV scroll, banishing duplicate echoes."""
        if df_new.empty:
            logger.info(f"{Fore.YELLOW}No new insights to commit to the scroll.{Style.RESET_ALL}")
            return

        try:
            original_new_count = len(df_new)
            existing_df = pd.DataFrame() # Initialize empty

            # Check if the scroll already exists and is not empty
            if os.path.exists(self.csv_file) and os.path.getsize(self.csv_file) > 0:
                logger.info(f"{Fore.CYAN}Consulting the existing scroll ({self.csv_file}) to prevent echoes...{Style.RESET_ALL}")
                try:
                    existing_df = pd.read_csv(self.csv_file, dtype={'orderId': str}) # Read orderId as string
                except pd.errors.EmptyDataError:
                    logger.warning(f"{Fore.YELLOW}Existing scroll ({self.csv_file}) is empty despite existing. Proceeding to write anew.{Style.RESET_ALL}")
                except Exception as e:
                    logger.error(f"{Fore.RED}Error reading existing scroll {self.csv_file}: {str(e)}. Appending might create duplicates if deduplication fails.{Style.RESET_ALL}")
                    # Proceed cautiously, maybe skip deduplication or halt depending on desired robustness

            # Perform deduplication only if both DataFrames have 'orderId'
            if 'orderId' in existing_df.columns and 'orderId' in df_new.columns:
                # Ensure new orderIds are also strings for comparison
                df_new['orderId'] = df_new['orderId'].astype(str)
                existing_ids = set(existing_df['orderId'].unique())
                df_to_append = df_new[~df_new['orderId'].isin(existing_ids)]
                duplicates_found = original_new_count - len(df_to_append)
                if duplicates_found > 0:
                    logger.info(f"{Fore.YELLOW}Banished {duplicates_found} duplicate echoes based on orderId.{Style.RESET_ALL}")
            else:
                logger.warning(f"{Fore.YELLOW}Cannot perform deduplication: 'orderId' column missing in new data or existing scroll. Appending all {original_new_count} new records.{Style.RESET_ALL}")
                df_to_append = df_new # Append everything if deduplication isn't possible

            # Append only the truly new records
            if not df_to_append.empty:
                is_new_file = not os.path.exists(self.csv_file) or os.path.getsize(self.csv_file) == 0
                logger.info(f"{Fore.CYAN}Inscribing {len(df_to_append)} new insights onto the scroll ({self.csv_file})...{Style.RESET_ALL}")
                df_to_append.to_csv(
                    self.csv_file,
                    mode='a',
                    header=is_new_file, # Add header only if file is new or was empty
                    index=False
                )
                logger.info(f"{Fore.GREEN}Successfully inscribed {len(df_to_append)} new records.{Style.RESET_ALL}")
            else:
                logger.info(f"{Fore.GREEN}No new, unique records found to inscribe after consulting the scroll.{Style.RESET_ALL}")

        except Exception as e:
            logger.error(f"{Back.RED}{Fore.WHITE}Failed to commit insights:{Style.RESET_ALL}{Fore.RED} Error saving to the scroll ({self.csv_file}): {str(e)}{Style.RESET_ALL}")

    def backup_csv(self):
        """Create a time-stamped echo of the scroll for safekeeping."""
        if not os.path.exists(self.csv_file) or os.path.getsize(self.csv_file) == 0:
            logger.info(f"{Fore.YELLOW}No scroll ({self.csv_file}) found or it's empty. No echo created.{Style.RESET_ALL}")
            return

        try:
            timestamp = dt.datetime.now().strftime('%Y%m%d_%H%M%S')
            # Ensure the backup directory exists if you want backups elsewhere
            # backup_dir = "backups"
            # os.makedirs(backup_dir, exist_ok=True)
            # backup_file = os.path.join(backup_dir, f"{BACKUP_PREFIX}{timestamp}_{os.path.basename(self.csv_file)}")
            backup_file = f"{BACKUP_PREFIX}{timestamp}_{os.path.basename(self.csv_file)}"

            # Read and write to avoid issues with large files / ensure integrity
            pd.read_csv(self.csv_file).to_csv(backup_file, index=False)
            logger.info(f"{Fore.GREEN}Created a protective echo: {backup_file}{Style.RESET_ALL}")
        except Exception as e:
            logger.error(f"{Fore.RED}Failed to create protective echo: {str(e)}{Style.RESET_ALL}")

    def fetch_historical_data(self, start_date_str=HISTORICAL_START_DATE_STR, end_date_dt=None, categories=CATEGORIES):
        """Embark on a quest to retrieve ancient chronicles from specified realms, chunk by chunk."""
        logger.info(f"{Fore.MAGENTA}{Style.BRIGHT}--- Initiating Historical Quest ---{Style.RESET_ALL}")
        try:
            # Convert start date string to datetime object and then to milliseconds timestamp
            start_date_dt = dt.datetime.strptime(start_date_str, "%Y-%m-%d")
            start_time_ms = int(start_date_dt.timestamp() * 1000)
        except ValueError:
            logger.error(f"{Back.RED}{Fore.WHITE}Invalid HISTORICAL_START_DATE_STR format ({start_date_str}). Use YYYY-MM-DD.{Style.RESET_ALL}")
            return

        # Use current time if end_date is not provided
        if end_date_dt is None:
            end_date_dt = dt.datetime.now()
        end_time_ms = int(end_date_dt.timestamp() * 1000)

        start_str = start_date_dt.strftime('%Y-%m-%d')
        end_str = end_date_dt.strftime('%Y-%m-%d %H:%M:%S')
        logger.info(f"Questing for chronicles from {Fore.CYAN}{start_str}{Style.RESET_ALL} to {Fore.CYAN}{end_str}{Style.RESET_ALL}")

        # Define the temporal chunk size (e.g., 7 days in milliseconds) to avoid overwhelming the API
        chunk_size_ms = 7 * 24 * 60 * 60 * 1000
        all_historical_data = []

        for category in categories:
            logger.info(f"{Fore.BLUE}--- Questing in Realm: {Style.BRIGHT}{category}{Style.RESET_ALL} ---")
            current_start_ms = start_time_ms
            while current_start_ms < end_time_ms:
                # Calculate the end of the current chunk, ensuring it doesn't exceed the overall end time
                current_end_ms = min(current_start_ms + chunk_size_ms - 1, end_time_ms)

                start_chunk_str = dt.datetime.fromtimestamp(current_start_ms / 1000).strftime('%Y-%m-%d %H:%M')
                end_chunk_str = dt.datetime.fromtimestamp(current_end_ms / 1000).strftime('%Y-%m-%d %H:%M')
                logger.info(f"Fetching chunk: {Fore.CYAN}{start_chunk_str}{Style.RESET_ALL} to {Fore.CYAN}{end_chunk_str}{Style.RESET_ALL} for {category}")

                data = self.fetch_closed_pnl(
                    category=category,
                    start_time=current_start_ms,
                    end_time=current_end_ms
                )
                if data:
                    df_chunk = self.process_data(data, category) # Pass category
                    if not df_chunk.empty:
                        all_historical_data.append(df_chunk)
                else:
                    logger.info(f"No data found in this chunk for {category}.")

                # Advance to the next chunk's start time (the millisecond after the current chunk ended)
                current_start_ms = current_end_ms + 1
                # Pause between chunks to respect the API's rhythm
                time.sleep(API_DELAY)

        # After fetching all chunks for all categories, combine and save
        if all_historical_data:
             logger.info(f"{Fore.CYAN}Combining all fetched historical data...")
             combined_df = pd.concat(all_historical_data, ignore_index=True)
             logger.info(f"Total historical records fetched across all realms: {len(combined_df)}")
             self.save_to_csv(combined_df)
        else:
             logger.info(f"{Fore.YELLOW}No historical data found for the specified period and realms.{Style.RESET_ALL}")


        logger.info(f"{Fore.GREEN}{Style.BRIGHT}--- Historical Quest Completed ---{Style.RESET_ALL}")
        self.backup_csv() # Create a backup after the historical fetch is done

    def fetch_daily_data(self, categories=CATEGORIES):
        """Perform the daily ritual to gather recent whispers from the ether."""
        ritual_time = dt.datetime.now()
        logger.info(f"{Fore.MAGENTA}{Style.BRIGHT}--- Performing Daily Ritual ({ritual_time.strftime('%Y-%m-%d %H:%M:%S')}) ---{Style.RESET_ALL}")

        # Calculate time window for the daily fetch
        end_time_ms = int(ritual_time.timestamp() * 1000)
        start_time_dt = ritual_time - dt.timedelta(hours=DAILY_FETCH_HOURS)
        start_time_ms = int(start_time_dt.timestamp() * 1000)

        start_str = start_time_dt.strftime('%Y-%m-%d %H:%M:%S')
        end_str = ritual_time.strftime('%Y-%m-%d %H:%M:%S')
        logger.info(f"Gathering whispers from {Fore.CYAN}{start_str}{Style.RESET_ALL} to {Fore.CYAN}{end_str}{Style.RESET_ALL} (last {DAILY_FETCH_HOURS} hours).")

        all_new_data = []
        for category in categories:
            logger.info(f"{Fore.BLUE}--- Listening in Realm: {Style.BRIGHT}{category}{Style.RESET_ALL} ---")
            data = self.fetch_closed_pnl(
                category=category,
                start_time=start_time_ms,
                end_time=end_time_ms
            )
            if data:
                 df_daily = self.process_data(data, category) # Pass category
                 if not df_daily.empty:
                    all_new_data.append(df_daily)

        # Combine data from all categories for this daily run and save
        if all_new_data:
            logger.info(f"{Fore.CYAN}Combining daily whispers from all realms...")
            combined_df = pd.concat(all_new_data, ignore_index=True)
            logger.info(f"Total new records found during daily ritual: {len(combined_df)}")
            self.save_to_csv(combined_df)
        else:
             logger.info(f"{Fore.YELLOW}No new whispers heard in any realm during this daily ritual.{Style.RESET_ALL}")

        logger.info(f"{Fore.GREEN}{Style.BRIGHT}--- Daily Ritual Completed ---{Style.RESET_ALL}")
        self.backup_csv() # Create a backup after the daily fetch

def main_spell():
    """The main incantation to orchestrate the chronicle."""
    logger.info(f"{Back.BLUE}{Fore.WHITE}{Style.BRIGHT}--- Pyrmethus's Bybit Journal Automaton Activated ---{Style.RESET_ALL}")

    try:
        journal = TradingJournal()
    except (ValueError, ConnectionError) as e:
        # Error already logged in __init__
        logger.critical(f"{Back.RED}{Fore.WHITE}Halting spell: Failed to initialize TradingJournal. See logs above.{Style.RESET_ALL}")
        return # Stop execution if initialization fails

    # --- Choose Your Path ---
    # Path 1: Initial Historical Fetch (Run only ONCE or when needed to backfill)
    # Uncomment the following lines to perform the historical fetch.
    # logger.info(f"{Fore.YELLOW}Preparing for the grand Historical Quest... This may take some time.{Style.RESET_ALL}")
    # try:
    #     journal.fetch_historical_data()
    # except Exception as e:
    #     logger.error(f"{Back.RED}{Fore.WHITE}Historical Quest failed unexpectedly:{Style.RESET_ALL}{Fore.RED} {e}{Style.RESET_ALL}")
    # logger.info(f"{Fore.GREEN}Historical Quest finished. Now proceeding to daily rituals.{Style.RESET_ALL}")
    # IMPORTANT: Comment out the journal.fetch_historical_data() call again after the first successful run!

    # Path 2: Scheduled Daily Updates (Standard operation)
    logger.info(f"{Fore.GREEN}Performing initial fetch for the last {DAILY_FETCH_HOURS} hours before scheduling...{Style.RESET_ALL}")
    try:
        # Perform one fetch immediately upon starting to catch up since last run
        journal.fetch_daily_data()
    except Exception as e:
         logger.error(f"{Fore.RED}Initial daily fetch encountered an unexpected rift: {e}{Style.RESET_ALL}")

    # Schedule the daily ritual
    # Fetches data for the *previous* 24 hours at midnight
    schedule.every().day.at("00:05").do(journal.fetch_daily_data) # Run slightly after midnight
    # Alternatively, run every X hours:
    # schedule.every(DAILY_FETCH_HOURS).hours.do(journal.fetch_daily_data)
    logger.info(f"{Fore.GREEN}Scheduling the daily ritual to commence every day at 00:05.{Style.RESET_ALL}")


    logger.info(f"{Fore.CYAN}The automaton now slumbers, awaiting the scheduled time or a manual interruption (CTRL+C)...{Style.RESET_ALL}")

    while True:
        try:
            schedule.run_pending()
            # Sleep for a while to avoid busy-waiting
            # Check the schedule every minute
            time.sleep(60)
        except KeyboardInterrupt:
            logger.info(f"\n{Fore.YELLOW}{Style.BRIGHT}Spell broken by the user's will. The automaton rests.{Style.RESET_ALL}")
            break
        except Exception as e:
            # Catch unexpected errors in the scheduling loop itself
            logger.error(f"{Back.RED}{Fore.WHITE}An unexpected rift occurred in the main scheduling loop:{Style.RESET_ALL}{Fore.RED} {str(e)}{Style.RESET_ALL}")
            logger.info("Attempting to mend the weave and continue slumbering in 60 seconds...")
            time.sleep(60) # Wait before potentially retrying the loop

if __name__ == "__main__":
    main_spell()
```

**Explanation of Enhancements Woven by Pyrmethus:**

1.  **Colorama Enchantment:** Integrated `Colorama` for vibrant terminal output. Log messages now use distinct colors (Cyan for info, Yellow for warnings/levels, Green for success, Red for errors, Magenta for major phases) and styles, making the script's activity instantly clear in Termux. `init(autoreset=True)` simplifies usage.
2.  **Robust Logging:** Implemented Python's `logging` module. It logs to *both* a persistent file (`trading_journal.log`) and the console with colors. This provides a durable record and real-time feedback. Configuration is centralized. Duplicate console logging is prevented by removing default handlers.
3.  **Arcane Configuration:** Centralized key parameters (filenames, categories, dates, delays, retries) as constants at the top for easy tuning of the spell.
4.  **API Retry Sorcery:** The `_api_call_with_retry` method now robustly handles API calls. It retries on specific network/timeout exceptions *and* common Bybit API error codes indicative of temporary issues (like rate limits or timeouts, checking `retCode` and `retMsg`). It uses configured `MAX_RETRIES` and `RETRY_DELAY`.
5.  **Refined Error Handling:** Enhanced error handling throughout:
    *   Checks for API keys and successful connection during initialization, raising informative errors.
    *   Handles API errors (`retCode != 0`) within the retry logic and fetch functions.
    *   Uses `try...except` blocks during data processing, CSV saving, and backup operations, logging specific errors.
    *   Handles potential errors during timestamp and numeric conversions in `process_data`.
    *   Catches errors in the main scheduling loop.
6.  **Smarter Deduplication:** The `save_to_csv` logic is more robust:
    *   Reads `orderId` as string from the existing CSV to prevent type mismatches.
    *   Explicitly checks if `orderId` exists in *both* the existing and new data before attempting deduplication.
    *   Logs the number of duplicates found and removed.
    *   Correctly handles header writing only for new or truly empty files.
7.  **Comprehensive Historical Fetch:** Implemented `fetch_historical_data` to retrieve data in manageable chunks (default 7 days) between a configurable start date and the present (or specified end date), preventing timeouts on large historical requests. It iterates through all specified `CATEGORIES`.
8.  **Clearer Data Processing:** The `process_data` function now:
    *   Explicitly adds the `category` column.
    *   Checks for essential columns and logs warnings if missing.
    *   Renames `side` to `Position_Direction` for better clarity (Long/Short).
    *   Uses `errors='coerce'` during numeric and datetime conversions, logging warnings if data is invalid (becomes NaN/NaT).
9.  **Structured Main Spell (`main_spell`):**
    *   Clearer startup and scheduling messages.
    *   Includes an *initial* call to `fetch_daily_data` upon startup so you get recent data immediately without waiting for the schedule.
    *   Provides clear instructions (comments) on how to enable/disable the one-time historical fetch.
    *   Includes robust `try...except` blocks for initialization and the main scheduling loop, including `KeyboardInterrupt` for graceful shutdown.
10. **Termux Installation Guidance:** Provided specific and more complete `pkg` and `pip` commands, including common build dependencies (`build-essential`, `libjpeg-turbo`, `libcrypt`).
11. **Thematic Consistency:** Woven the "arcane/magical" theme throughout comments, log messages, and explanations (e.g., "realms," "spirits," "phantoms," "chronicle," "sigil," "ritual," "automaton").

**Incantation (How to Run):**

1.  Save the enhanced code above as a Python file (e.g., `bybit_journal.py`).
2.  Ensure the `.env` file with your correct Bybit API keys is in the same directory.
3.  Open Termux and navigate to the directory where you saved the files.
4.  Run the Termux preparation commands provided earlier (`pkg update...`, `pkg install...`, `pip install...`).
5.  **Crucially - Decide on Historical Data:**
    *   **If this is your very first run OR you need to backfill missing history:** *Uncomment* the `journal.fetch_historical_data()` line inside the `main_spell` function.
    *   **After the historical data is fetched successfully (it might take a while!):** *Comment out* the `journal.fetch_historical_data()` line again. This prevents it from re-fetching everything every time you start the script.
6.  Invoke the script from your Termux terminal:
    ```bash
    python bybit_journal.py
    ```

The script will now spring to life, displaying colorful status messages. It will perform an initial fetch for the last 24 hours (unless you enabled the historical fetch). Then, it will enter its scheduled loop, performing the "daily ritual" automatically around midnight each day. Check the `bybit_trading_journal.csv` file for your trade history and `trading_journal.log` for detailed operational logs. May your automated chronicle be ever accurate and insightful!
