#!/usr/bin/env python

"""
Technical Indicators Module (v2.0 - Integrated with Bybit Trading Enchanted)

Calculates technical indicators using pandas_ta, with support for incremental updates
for real-time trading via WebSocket data. Optimized for use with BybitHelper in a
Termux environment.

Key Features:
- Wrappers for pandas_ta indicators (SMA, EMA, RSI, ATR, etc.).
- Ehlers Volumetric Trend (EVT) with VWMA and SuperSmoother.
- Standard and Fibonacci pivot points and support/resistance levels.
- Incremental indicator updates for WebSocket-driven data.
- Configuration parsing compatible with AppConfig.
- Robust error handling and logging aligned with BybitHelper.

Assumes input DataFrame has columns: 'open', 'high', 'low', 'close', 'volume'
and a datetime index (UTC).
"""

import logging
import sys
from typing import Any, Dict, Optional, Tuple
import numpy as np
import pandas as pd

try:
    import pandas_ta as ta
except ImportError:
    print("Error: pandas_ta library not found. Install: pip install pandas_ta", file=sys.stderr)
    sys.exit(1)

# --- Logger Setup ---
logger = logging.getLogger(__name__)
if not logger.hasHandlers():
    handler = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

# --- Constants ---
MIN_PERIODS_DEFAULT = 50

# --- Pivot Point Calculations ---
def calculate_standard_pivot_points(high: float, low: float, close: float) -> Dict[str, float]:
    """Calculate standard pivot points for the next period."""
    if not all(isinstance(v, (int, float)) and not np.isnan(v) for v in [high, low, close]):
        logger.warning("Invalid input for standard pivot points.")
        return {}
    if low > high:
        logger.warning(f"Low ({low}) > High ({high}) for pivot calculation.")
    try:
        pivot = (high + low + close) / 3.0
        return {
            "PP": pivot,
            "S1": (2 * pivot) - high,
            "R1": (2 * pivot) - low,
            "S2": pivot - (high - low),
            "R2": pivot + (high - low),
            "S3": low - 2 * (high - pivot),
            "R3": high + 2 * (pivot - low)
        }
    except Exception as e:
        logger.error(f"Error calculating standard pivot points: {e}", exc_info=True)
        return {}

def calculate_fib_pivot_points(high: float, low: float, close: float) -> Dict[str, float]:
    """Calculate Fibonacci pivot points for the next period."""
    if not all(isinstance(v, (int, float)) and not np.isnan(v) for v in [high, low, close]):
        logger.warning("Invalid input for Fibonacci pivot points.")
        return {}
    if low > high:
        logger.warning(f"Low ({low}) > High ({high}) for Fib pivot calculation.")
    try:
        pivot = (high + low + close) / 3.0
        fib_range = high - low
        if abs(fib_range) < 1e-9:
            logger.warning("Zero or near-zero range for Fibonacci pivots.")
            return {"PP": pivot}
        return {
            "PP": pivot,
            "S1": pivot - (0.382 * fib_range),
            "R1": pivot + (0.382 * fib_range),
            "S2": pivot - (0.618 * fib_range),
            "R2": pivot + (0.618 * fib_range),
            "S3": pivot - (1.000 * fib_range),
            "R3": pivot + (1.000 * fib_range)
        }
    except Exception as e:
        logger.error(f"Error calculating Fibonacci pivot points: {e}", exc_info=True)
        return {}

# --- Support/Resistance Levels ---
def calculate_levels(df_period: pd.DataFrame, current_price: Optional[float] = None) -> Dict[str, Any]:
    """Calculate support/resistance levels including pivots and Fibonacci retracements."""
    levels = {
        "support": {},
        "resistance": {},
        "pivot": None,
        "fib_retracements": {},
        "standard_pivots": {},
        "fib_pivots": {}
    }
    required_cols = ["high", "low", "close"]
    if not all(col in df_period.columns for col in required_cols) or df_period.empty:
        logger.warning("Missing HLC columns or empty DataFrame for levels calculation.")
        return levels
    try:
        if len(df_period) >= 2:
            prev_high = df_period["high"].iloc[-2]
            prev_low = df_period["low"].iloc[-2]
            prev_close = df_period["close"].iloc[-2]
            levels["standard_pivots"] = calculate_standard_pivot_points(prev_high, prev_low, prev_close)
            levels["fib_pivots"] = calculate_fib_pivot_points(prev_high, prev_low, prev_close)
            levels["pivot"] = levels["standard_pivots"].get("PP", levels["fib_pivots"].get("PP"))

        period_high = df_period["high"].max()
        period_low = df_period["low"].min()
        period_diff = period_high - period_low
        if abs(period_diff) > 1e-9:
            levels["fib_retracements"] = {
                "High": period_high,
                "Fib 78.6%": period_low + period_diff * 0.786,
                "Fib 61.8%": period_low + period_diff * 0.618,
                "Fib 50.0%": period_low + period_diff * 0.5,
                "Fib 38.2%": period_low + period_diff * 0.382,
                "Fib 23.6%": period_low + period_diff * 0.236,
                "Low": period_low
            }

        if current_price is not None:
            all_levels = {
                **{f"Std {k}": v for k, v in levels["standard_pivots"].items()},
                **{f"Fib {k}": v for k, v in levels["fib_pivots"].items() if k != "PP"},
                **levels["fib_retracements"]
            }
            for label, value in all_levels.items():
                if isinstance(value, (int, float)):
                    if value < current_price:
                        levels["support"][label] = value
                    elif value > current_price:
                        levels["resistance"][label] = value

        levels["support"] = dict(sorted(levels["support"].items(), key=lambda x: x[1], reverse=True))
        levels["resistance"] = dict(sorted(levels["resistance"].items(), key=lambda x: x[1]))
        return levels
    except Exception as e:
        logger.error(f"Error calculating levels: {e}", exc_info=True)
        return levels

# --- Custom Indicators ---
def calculate_vwma(close: pd.Series, volume: pd.Series, length: int) -> Optional[pd.Series]:
    """Calculate Volume Weighted Moving Average (VWMA)."""
    if close.empty or volume.empty or length <= 0 or len(close) < length or len(close) != len(volume):
        logger.warning(f"Invalid inputs for VWMA (length={length}).")
        return None
    try:
        pv = close * volume
        cumulative_pv = pv.rolling(window=length, min_periods=length).sum()
        cumulative_vol = volume.rolling(window=length, min_periods=length).sum()
        vwma = cumulative_pv / cumulative_vol
        vwma.replace([np.inf, -np.inf], np.nan, inplace=True)
        vwma.name = f"VWMA_{length}"
        return vwma
    except Exception as e:
        logger.error(f"Error calculating VWMA(length={length}): {e}", exc_info=True)
        return None

def ehlers_volumetric_trend(df: pd.DataFrame, length: int, multiplier: float) -> pd.DataFrame:
    """Calculate Ehlers Volumetric Trend with VWMA and SuperSmoother."""
    required_cols = ["close", "volume"]
    if not all(col in df.columns for col in required_cols) or df.empty or length <= 1 or multiplier <= 0:
        logger.warning(f"EVT skipped: Invalid params (len={length}, mult={multiplier}).")
        return df
    df_out = df.copy()
    vwma_col = f"vwma_{length}"
    smooth_col = f"smooth_vwma_{length}"
    trend_col = f"evt_trend_{length}"
    buy_col = f"evt_buy_{length}"
    sell_col = f"evt_sell_{length}"
    try:
        vwma = calculate_vwma(df_out["close"], df_out["volume"], length)
        if vwma is None:
            raise ValueError(f"VWMA calculation failed for EVT (length={length})")
        df_out[vwma_col] = vwma

        # SuperSmoother Filter
        a = np.exp(-1.414 * np.pi / length)
        b = 2 * a * np.cos(1.414 * np.pi / length)
        c2, c3, c1 = b, -a * a, 1 - b + a * a
        smoothed = pd.Series(np.nan, index=df_out.index, dtype=float)
        vwma_vals = df_out[vwma_col].values
        for i in range(2, len(df_out)):
            if not np.isnan(vwma_vals[i]):
                sm1 = smoothed.iloc[i-1] if pd.notna(smoothed.iloc[i-1]) else vwma_vals[i-1]
                sm2 = smoothed.iloc[i-2] if pd.notna(smoothed.iloc[i-2]) else vwma_vals[i-2]
                smoothed.iloc[i] = c1 * vwma_vals[i] + c2 * sm1 + c3 * sm2
        df_out[smooth_col] = smoothed

        # Trend Determination
        mult_h, mult_l = 1.0 + multiplier / 100.0, 1.0 - multiplier / 100.0
        shifted_smooth = df_out[smooth_col].shift(1)
        trend = pd.Series(0, index=df_out.index, dtype=int)
        up_trend = (df_out[smooth_col] > shifted_smooth * mult_h) & pd.notna(df_out[smooth_col]) & pd.notna(shifted_smooth)
        down_trend = (df_out[smooth_col] < shifted_smooth * mult_l) & pd.notna(df_out[smooth_col]) & pd.notna(shifted_smooth)
        for i in range(len(df_out)):
            trend.iloc[i] = 1 if up_trend.iloc[i] else -1 if down_trend.iloc[i] else trend.iloc[i-1] if i > 0 else 0
        df_out[trend_col] = trend
        df_out[buy_col] = (df_out[trend_col] == 1) & (df_out[trend_col].shift(1).fillna(0) != 1)
        df_out[sell_col] = (df_out[trend_col] == -1) & (df_out[trend_col].shift(1).fillna(0) != -1)

        logger.debug(f"EVT calculated (len={length}, mult={multiplier}).")
        return df_out
    except Exception as e:
        logger.error(f"Error in EVT(len={length}, mult={multiplier}): {e}", exc_info=True)
        df_out[vwma_col] = df_out[smooth_col] = df_out[trend_col] = df_out[buy_col] = df_out[sell_col] = np.nan
        return df_out

def update_indicators_incrementally(df: pd.DataFrame, config: Dict[str, Any], prev_df: Optional[pd.DataFrame] = None) -> pd.DataFrame:
    """Update indicators incrementally for the latest data point."""
    if df.empty or not all(col in df.columns for col in ["open", "high", "low", "close", "volume"]):
        logger.warning("Invalid DataFrame for incremental update.")
        return df
    df_out = df.copy()
    settings = config.get("indicator_settings", {})
    flags = config.get("analysis_flags", {})
    strategy_params = config.get("strategy_params", {}).get("ehlers_volumetric", {})

    try:
        if flags.get("use_evt"):
            evt_len = strategy_params.get("evt_length", settings.get("evt_length", 7))
            evt_mult = strategy_params.get("evt_multiplier", settings.get("evt_multiplier", 2.5))
            if prev_df is not None and len(prev_df) >= evt_len:
                df_combined = pd.concat([prev_df.iloc[-(evt_len-1):], df_out], ignore_index=True)
                df_combined = ehlers_volumetric_trend(df_combined, evt_len, float(evt_mult))
                df_out = df_combined.iloc[-len(df_out):].copy()
            else:
                df_out = ehlers_volumetric_trend(df_out, evt_len, float(evt_mult))

        if flags.get("use_atr"):
            atr_len = settings.get("atr_period", 14)
            if prev_df is not None and len(prev_df) >= atr_len:
                df_combined = pd.concat([prev_df.iloc[-(atr_len-1):], df_out], ignore_index=True)
                atr_result = ta.atr(df_combined["high"], df_combined["low"], df_combined["close"], length=atr_len)
                df_out[f"ATRr_{atr_len}"] = atr_result.iloc[-len(df_out):]
            else:
                df_out[f"ATRr_{atr_len}"] = ta.atr(df_out["high"], df_out["low"], df_out["close"], length=atr_len)

        return df_out
    except Exception as e:
        logger.error(f"Error in incremental indicator update: {e}", exc_info=True)
        return df_out

# --- Master Indicator Calculation ---
def calculate_all_indicators(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
    """Calculate all enabled indicators based on config."""
    if df.empty or not all(col in df.columns for col in ["open", "high", "low", "close", "volume"]):
        logger.error("Invalid DataFrame for indicator calculation.")
        return pd.DataFrame()
    df_out = df.copy()
    settings = config.get("indicator_settings", {})
    flags = config.get("analysis_flags", {})
    min_rows = settings.get("min_data_periods", MIN_PERIODS_DEFAULT)
    strategy_params = config.get("strategy_params", {}).get("ehlers_volumetric", {})

    if len(df_out.dropna(subset=["open", "high", "low", "close", "volume"])) < min_rows:
        logger.warning(f"Insufficient valid rows ({len(df_out)}) for reliable calculations. Need {min_rows}.")
    try:
        if flags.get("use_sma"):
            for period in [settings.get("sma_short_period", 10), settings.get("sma_long_period", 50)]:
                if period > 0:
                    df_out[f"SMA_{period}"] = ta.sma(df_out["close"], length=period)
        if flags.get("use_ema"):
            for period in [settings.get("ema_short_period", 12), settings.get("ema_long_period", 26)]:
                if period > 0:
                    df_out[f"EMA_{period}"] = ta.ema(df_out["close"], length=period)
        if flags.get("use_rsi"):
            rsi_len = settings.get("rsi_period", 14)
            if rsi_len > 0:
                df_out[f"RSI_{rsi_len}"] = ta.rsi(df_out["close"], length=rsi_len)
        if flags.get("use_atr"):
            atr_len = settings.get("atr_period", 14)
            if atr_len > 0:
                df_out[f"ATRr_{atr_len}"] = ta.atr(df_out["high"], df_out["low"], df_out["close"], length=atr_len)
        if flags.get("use_evt"):
            evt_len = strategy_params.get("evt_length", settings.get("evt_length", 7))
            evt_mult = strategy_params.get("evt_multiplier", settings.get("evt_multiplier", 2.5))
            df_out = ehlers_volumetric_trend(df_out, evt_len, float(evt_mult))

        logger.debug(f"Calculated indicators. DataFrame shape: {df_out.shape}")
        return df_out
    except Exception as e:
        logger.error(f"Error calculating indicators: {e}", exc_info=True)
        return df_out

# --- Standalone Demo ---
if __name__ == "__main__":
    logger.setLevel(logging.DEBUG)
    test_config = {
        "indicator_settings": {
            "min_data_periods": 50,
            "sma_short_period": 10,
            "sma_long_period": 50,
            "ema_short_period": 12,
            "ema_long_period": 26,
            "rsi_period": 14,
            "atr_period": 14,
            "evt_length": 7,
            "evt_multiplier": 2.5
        },
        "analysis_flags": {
            "use_sma": True,
            "use_ema": True,
            "use_rsi": True,
            "use_atr": True,
            "use_evt": True
        },
        "strategy_params": {
            "ehlers_volumetric": {
                "evt_length": 7,
                "evt_multiplier": 2.5
            }
        }
    }
    periods = 200
    prices = 1000 * np.exp(np.cumsum(np.random.normal(loc=0.0001, scale=0.01, size=periods)))
    df_test = pd.DataFrame({
        "timestamp": pd.date_range(start="2025-01-01", periods=periods, freq="5min"),
        "open": prices[:-1],
        "close": prices[1:],
        "high": prices[1:] * (1 + np.random.uniform(0, 0.01, periods-1)),
        "low": prices[1:] * (1 - np.random.uniform(0, 0.01, periods-1)),
        "volume": np.random.uniform(100, 2000, periods-1)
    }).set_index("timestamp")
    df_test["high"] = np.maximum.reduce([df_test["open"], df_test["close"], df_test["high"]])
    df_test["low"] = np.minimum.reduce([df_test["open"], df_test["close"], df_test["low"]])
    df_results = calculate_all_indicators(df_test, test_config)
    print(f"Output shape: {df_results.shape}")
    print(f"Columns: {df_results.columns.tolist()}")
    print(f"Last row:\n{df_results.iloc[-1]}")Hark, seeker! You wish to weave the concepts of **Order Blocks** and **Support/Resistance** into our indicator tapestry. These are potent forms of market structure analysis, revealing potential zones of supply and demand and historical price barriers.

I shall conjure the necessary spells to identify these patterns. We will focus on:

1.  **Swing High/Low Support & Resistance**: Identifying recent significant turning points in price.
2.  **Basic Order Block Identification**: Marking candles that precede strong moves, potentially indicating institutional interest.

Let's enhance the `indicators.py` scroll:

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Indicators Module (v3.4 - Pyrmethus Weave: Structure)

Contains the alchemical formulas (technical indicators) for trading strategies,
optimized for use with the BybitHelper Arcanum. Uses pandas_ta for efficiency
and includes custom Ehlers Volumetric Trend (EVT), manual Pivot Points,
Swing High/Low Support/Resistance, and basic Order Block identification.

Provides:
- calculate_indicators: Processes a full DataFrame scroll.
- update_indicators: Efficiently updates indicators for new candle whispers.

Indicators Included:
EVT, ATR, SMA, HMA, ZEMA, Supertrend, MACD, RSI, Fisher(RSI), Momentum,
StochRSI, ADX, Volume SMA, Standard Pivots (Daily), Fibonacci Pivots (Daily),
Swing High/Low S/R, Basic Order Blocks.

Termux Dependencies:
pkg install python numpy pandas
pip install pandas_ta colorama
"""

import logging
from typing import Any, Dict, Optional

import numpy as np # Summoned for numerical conjurations
import pandas as pd
import pandas_ta as ta
from colorama import init as colorama_init, Fore, Style # For vibrant terminal echoes

# Initialize Colorama for mystical hues
colorama_init(autoreset=True)

# --- Configuration Import ---
# Attempt to import the AppConfig spellbook from the main helper scroll.
try:
    from bybit_trading_enhanced import AppConfig
except ImportError:
    print(f"{Fore.RED + Style.BRIGHT}Fatal Error:{Style.RESET_ALL} {Fore.YELLOW}Cannot import AppConfig from bybit_trading_enhanced.py.{Style.RESET_ALL}")
    print(f"{Fore.YELLOW}Ensure bybit_trading_enhanced.py is accessible and defines AppConfig.{Style.RESET_ALL}")
    # Define a dummy AppConfig locally ONLY for static analysis or isolated testing.
    class AppConfig:
        # Provide default values matching the real AppConfig structure
        evt_length = 7
        evt_multiplier = 2.5
        atr_period = 14
        sma_short = 10
        sma_long = 50
        hma_period = 9
        zema_period = 14
        supertrend_period = 10
        supertrend_multiplier = 3.0
        macd_fast = 12
        macd_slow = 26
        macd_signal = 9
        rsi_period = 14
        fisher_rsi_period = 9
        momentum_period = 10
        stochrsi_length = 14
        stochrsi_rsi_length = 14
        stochrsi_k = 3
        stochrsi_d = 3
        adx_period = 14
        volume_sma_period = 20
        symbol = "BTCUSDT"
        # --- New Parameters for Structure ---
        swing_sr_window = 10 # Bars left/right to confirm swing point
        ob_atr_threshold = 1.5 # Min ATR multiple for move after OB candle
        ob_wick_factor = 0.5 # Max wick size relative to body for OB candle (optional filter)

        # Helper for dummy config display
        def model_dump_json(self, indent=None):
            import json
            attrs = {k: v for k, v in self.__dict__.items() if not k.startswith('_')}
            return json.dumps(attrs, indent=indent)

    print(f"{Fore.YELLOW + Style.BRIGHT}Warning:{Style.RESET_ALL} {Fore.YELLOW}Using dummy AppConfig for indicators.py. This is for linting/testing only.{Style.RESET_ALL}")


# Setup logger for this module's whispers
logger = logging.getLogger(__name__)
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')


# --- Pivot Point Calculation Spell (Unchanged) ---
def _calculate_daily_pivot_points(daily_df: pd.DataFrame) -> pd.DataFrame:
    """Calculates Standard and Fibonacci Pivot Points based on previous day's HLC."""
    logger.debug(f"{Fore.CYAN}# Calculating daily pivot points...{Style.RESET_ALL}")
    if not all(col in daily_df.columns for col in ['high', 'low', 'close']):
        logger.error(f"{Fore.RED}Daily DataFrame missing required columns (high, low, close) for pivot calculation.{Style.RESET_ALL}")
        return pd.DataFrame()
    daily_df = daily_df.sort_index()
    prev_high = daily_df['high'].shift(1)
    prev_low = daily_df['low'].shift(1)
    prev_close = daily_df['close'].shift(1)
    pp = (prev_high + prev_low + prev_close) / 3
    r1 = (2 * pp) - prev_low; s1 = (2 * pp) - prev_high
    r2 = pp + (prev_high - prev_low); s2 = pp - (prev_high - prev_low)
    r3 = prev_high + (2 * (pp - prev_low)); s3 = prev_low - (2 * (prev_high - pp))
    fib_range = prev_high - prev_low
    fr1 = pp + (0.382 * fib_range); fs1 = pp - (0.382 * fib_range)
    fr2 = pp + (0.618 * fib_range); fs2 = pp - (0.618 * fib_range)
    fr3 = pp + (1.000 * fib_range); fs3 = pp - (1.000 * fib_range)
    pivots = pd.DataFrame({
        'PP': pp, 'S1': s1, 'R1': r1, 'S2': s2, 'R2': r2, 'S3': s3, 'R3': r3,
        'FS1': fs1, 'FR1': fr1, 'FS2': fs2, 'FR2': fr2, 'FS3': fs3, 'FR3': fr3
    }, index=daily_df.index)
    pivots = pivots.iloc[1:]
    logger.debug(f"{Fore.GREEN}Calculated pivots for {len(pivots)} days.{Style.RESET_ALL}")
    return pivots

# --- Swing High/Low Support & Resistance Spell ---
def _calculate_swing_sr(df: pd.DataFrame, window: int) -> pd.DataFrame:
    """
    Identifies Swing Highs and Lows to mark potential Support and Resistance levels.
    Looks for peaks and troughs within a defined window.

    Args:
        df: DataFrame with 'high' and 'low' columns.
        window: Number of bars to the left and right to confirm a swing point.

    Returns:
        DataFrame with added columns:
        - 'Is_Swing_High': Boolean, True if the bar is a swing high.
        - 'Is_Swing_Low': Boolean, True if the bar is a swing low.
        - 'Resistance': Price level of the last identified swing high (forward-filled).
        - 'Support': Price level of the last identified swing low (forward-filled).
    """
    logger.debug(f"{Fore.CYAN}# Identifying Swing High/Low Support & Resistance (window: {window})...{Style.RESET_ALL}")
    if window <= 0:
        logger.warning(f"{Fore.YELLOW}Swing S/R window must be positive. Skipping calculation.{Style.RESET_ALL}")
        df['Is_Swing_High'] = False
        df['Is_Swing_Low'] = False
        df['Resistance'] = np.nan
        df['Support'] = np.nan
        return df

    # Use rolling max/min to find local peaks/troughs efficiently
    # A high is a swing high if it's the highest high in a 2*window + 1 period centered on it.
    # Similarly for swing lows.
    local_max = df['high'].rolling(window * 2 + 1, center=True, min_periods=window + 1).max()
    local_min = df['low'].rolling(window * 2 + 1, center=True, min_periods=window + 1).min()

    df['Is_Swing_High'] = (df['high'] == local_max)
    df['Is_Swing_Low'] = (df['low'] == local_min)

    # Store the price level of the swing points
    df['Swing_High_Level'] = df['high'].where(df['Is_Swing_High'])
    df['Swing_Low_Level'] = df['low'].where(df['Is_Swing_Low'])

    # Forward fill to get the current active S/R levels
    # A new swing point might invalidate the previous one immediately, ffill captures this.
    df['Resistance'] = df['Swing_High_Level'].ffill()
    df['Support'] = df['Swing_Low_Level'].ffill()

    # Clean up intermediate columns
    # df.drop(columns=['Swing_High_Level', 'Swing_Low_Level'], inplace=True)
    logger.debug(f"{Fore.GREEN}Swing S/R calculation complete.{Style.RESET_ALL}")
    return df

# --- Basic Order Block Identification Spell ---
def _calculate_order_blocks(df: pd.DataFrame, atr_col: str, atr_threshold: float, wick_factor: Optional[float] = None) -> pd.DataFrame:
    """
    Identifies potential Bullish and Bearish Order Block candles.
    Looks for specific candles followed by a strong move (relative to ATR).

    Args:
        df: DataFrame with 'open', 'high', 'low', 'close', and the ATR column.
        atr_col: Name of the Average True Range column (e.g., 'ATR_14').
        atr_threshold: Multiplier for ATR to define a "strong move" on the next candle.
        wick_factor: Optional. If provided, filters OB candles where wick size relative
                     to body size exceeds this factor (e.g., 0.5 means wicks <= 50% of body).

    Returns:
        DataFrame with added columns:
        - 'Is_Bullish_OB': Boolean, True if the candle is a potential Bullish OB.
        - 'Is_Bearish_OB': Boolean, True if the candle is a potential Bearish OB.
        - 'OB_High': High price of the identified OB candle.
        - 'OB_Low': Low price of the identified OB candle.
        - 'OB_Type': 'Bullish' or 'Bearish' if an OB, else NaN.
    """
    logger.debug(f"{Fore.CYAN}# Identifying potential Order Blocks (ATR Threshold: {atr_threshold}, Wick Factor: {wick_factor})...{Style.RESET_ALL}")
    df['Is_Bullish_OB'] = False
    df['Is_Bearish_OB'] = False
    df['OB_High'] = np.nan
    df['OB_Low'] = np.nan
    df['OB_Type'] = np.nan

    if atr_col not in df.columns:
        logger.error(f"{Fore.RED}ATR column '{atr_col}' not found. Cannot calculate Order Blocks.{Style.RESET_ALL}")
        return df

    # Shift data to compare current candle with the *next* candle's move
    df_shifted = df.shift(-1)
    atr_val = df[atr_col]
    next_move_size = (df_shifted['high'] - df_shifted['low']).abs()
    min_move = atr_threshold * atr_val

    # --- Identify Potential Bullish OBs ---
    # Condition 1: Current candle is bearish (close < open)
    # Condition 2: Next candle is bullish (close > open)
    # Condition 3: Next candle's move is significant (range > threshold * ATR)
    # Condition 4: Next candle closes above the high of the current (potential OB) candle
    # Condition 5 (Optional): Current candle wick ratio check
    bullish_ob_cond = (
        (df['close'] < df['open']) &
        (df_shifted['close'] > df_shifted['open']) &
        (next_move_size > min_move) &
        (df_shifted['close'] > df['high'])
    )

    # --- Identify Potential Bearish OBs ---
    # Condition 1: Current candle is bullish (close > open)
    # Condition 2: Next candle is bearish (close < open)
    # Condition 3: Next candle's move is significant (range > threshold * ATR)
    # Condition 4: Next candle closes below the low of the current (potential OB) candle
    # Condition 5 (Optional): Current candle wick ratio check
    bearish_ob_cond = (
        (df['close'] > df['open']) &
        (df_shifted['close'] < df_shifted['open']) &
        (next_move_size > min_move) &
        (df_shifted['close'] < df['low'])
    )

    # --- Optional Wick Filter ---
    if wick_factor is not None:
        body_size = (df['close'] - df['open']).abs().replace(0, 0.00001) # Avoid division by zero for dojis
        upper_wick = df['high'] - df[['open', 'close']].max(axis=1)
        lower_wick = df[['open', 'close']].min(axis=1) - df['low']

        # Bullish OB (down candle): Check upper wick (against bearish body)
        bullish_wick_cond = (upper_wick / body_size) <= wick_factor
        # Bearish OB (up candle): Check lower wick (against bullish body)
        bearish_wick_cond = (lower_wick / body_size) <= wick_factor

        bullish_ob_cond = bullish_ob_cond & bullish_wick_cond
        bearish_ob_cond = bearish_ob_cond & bearish_wick_cond


    # --- Apply Conditions ---
    df.loc[bullish_ob_cond, 'Is_Bullish_OB'] = True
    df.loc[bullish_ob_cond, 'OB_High'] = df['high']
    df.loc[bullish_ob_cond, 'OB_Low'] = df['low']
    df.loc[bullish_ob_cond, 'OB_Type'] = 'Bullish'

    df.loc[bearish_ob_cond, 'Is_Bearish_OB'] = True
    df.loc[bearish_ob_cond, 'OB_High'] = df['high']
    df.loc[bearish_ob_cond, 'OB_Low'] = df['low']
    df.loc[bearish_ob_cond, 'OB_Type'] = 'Bearish'

    # Note: This identifies the *candle* that forms the OB.
    # A trading strategy would need to track these OB ranges (OB_High, OB_Low)
    # and check for mitigation (price returning to the range).
    logger.debug(f"{Fore.GREEN}Order Block identification complete.{Style.RESET_ALL}")
    return df


# --- Main Indicator Calculation ---
def calculate_indicators(df: pd.DataFrame, config: AppConfig, daily_df: Optional[pd.DataFrame] = None) -> pd.DataFrame:
    """
    Calculates all technical indicators and structural elements (Pivots, S/R, OBs)
    for the given OHLCV DataFrame scroll. The grand conjuration, now with structure!

    Args:
        df: Primary DataFrame scroll (e.g., 5m).
        config: AppConfig spellbook containing indicator parameters.
        daily_df: Optional DataFrame scroll with Daily OHLCV data for pivot calculations.

    Returns:
        DataFrame scroll with original columns plus calculated indicator/structure columns.
    """
    logger.debug(f"{Fore.MAGENTA}Calculating indicators & structure for DataFrame with {len(df)} rows. Daily pivots: {'Yes' if daily_df is not None else 'No'}{Style.RESET_ALL}")
    required_columns = {"timestamp", "open", "high", "low", "close", "volume"}
    if not required_columns.issubset(df.columns):
        logger.error(f"{Fore.RED}Primary DataFrame scroll missing required columns. Need: {required_columns}. Got: {list(df.columns)}{Style.RESET_ALL}")
        return pd.DataFrame()
    if df.empty:
        logger.warning(f"{Fore.YELLOW}Input primary DataFrame scroll is empty. Cannot conjure indicators.{Style.RESET_ALL}")
        return pd.DataFrame()

    # Ensure timestamp is datetime and make a copy
    try:
        df_processed = df.copy()
        if not pd.api.types.is_datetime64_any_dtype(df_processed['timestamp']):
            logger.debug(f"{Fore.CYAN}# Primary timestamp column requires temporal alignment... converting.{Style.RESET_ALL}")
            df_processed['timestamp'] = pd.to_datetime(df_processed['timestamp'])
        df_indexed = df_processed.set_index('timestamp').sort_index()
    except Exception as e:
        logger.error(f"{Fore.RED}Failed to process primary timestamp column: {e}{Style.RESET_ALL}", exc_info=True)
        return pd.DataFrame()

    try:
        # --- Custom Ehlers Volumetric Trend (EVT) Calculation ---
        logger.debug(f"{Fore.CYAN}# Conjuring Ehlers Volumetric Trend (EVT)...{Style.RESET_ALL}")
        vol_trend = df_indexed['close'].rolling(config.evt_length).mean() * \
                    df_indexed['volume'].rolling(config.evt_length).mean() * \
                    config.evt_multiplier
        df_indexed[f'EVT_{config.evt_length}'] = ta.ema(vol_trend, length=config.evt_length)
        df_indexed[f'EVT_Signal_{config.evt_length}'] = ta.ema(df_indexed[f'EVT_{config.evt_length}'], length=9)
        df_indexed[f'EVT_Trend_Up_{config.evt_length}'] = df_indexed[f'EVT_{config.evt_length}'] > df_indexed[f'EVT_Signal_{config.evt_length}']
        df_indexed[f'EVT_Trend_Down_{config.evt_length}'] = df_indexed[f'EVT_{config.evt_length}'] < df_indexed[f'EVT_Signal_{config.evt_length}']

        # --- Define pandas_ta Strategy - Core Mathematical Indicators ---
        rsi_period = getattr(config, 'rsi_period', 14)
        fisher_rsi_period = getattr(config, 'fisher_rsi_period', 9)
        momentum_period = getattr(config, 'momentum_period', 10)
        hma_period = getattr(config, 'hma_period', 9)
        zema_period = getattr(config, 'zema_period', 14)
        atr_period = getattr(config, 'atr_period', 14) # Ensure ATR period is accessible

        ta_strategy = ta.Strategy(
            name="Pyrmethus Core Indicators",
            description="ATR, SMAs, HMA, ZEMA, Supertrend, MACD, RSI, Momentum, StochRSI, ADX, Volume SMA",
            ta=[
                {"kind": "atr", "length": atr_period}, # Crucial for OBs
                {"kind": "sma", "length": config.sma_short},
                {"kind": "sma", "length": config.sma_long},
                {"kind": "hma", "length": hma_period},
                {"kind": "zlma", "length": zema_period},
                {"kind": "supertrend", "length": config.supertrend_period, "multiplier": config.supertrend_multiplier},
                {"kind": "macd", "fast": config.macd_fast, "slow": config.macd_slow, "signal": config.macd_signal},
                {"kind": "rsi", "length": rsi_period},
                {"kind": "mom", "length": momentum_period},
                {"kind": "stochrsi", "length": config.stochrsi_length, "rsi_length": config.stochrsi_rsi_length, "k": config.stochrsi_k, "d": config.stochrsi_d},
                {"kind": "adx", "length": config.adx_period},
                {"kind": "sma", "close": "volume", "length": config.volume_sma_period, "prefix": "VOLUME"},
            ]
        )

        # --- Apply the Core TA Strategy ---
        logger.debug(f"{Fore.CYAN}# Applying core pandas_ta strategy...{Style.RESET_ALL}")
        df_indexed.ta.strategy(ta_strategy)
        logger.debug(f"{Fore.GREEN}Core pandas_ta strategy application complete.{Style.RESET_ALL}")

        # --- Calculate Fisher Transform on RSI ---
        rsi_col_name = f'RSI_{rsi_period}'
        if rsi_col_name in df_indexed.columns:
             logger.debug(f"{Fore.CYAN}# Calculating Fisher Transform on {rsi_col_name}...{Style.RESET_ALL}")
             fisher_result = ta.fisher(df_indexed[rsi_col_name].fillna(50), length=fisher_rsi_period, signal=1)
             if fisher_result is not None and not fisher_result.empty and fisher_result.shape[1] >= 2:
                 df_indexed['Fisher_RSI'] = fisher_result.iloc[:, 0]
                 df_indexed['Fisher_RSI_Signal'] = fisher_result.iloc[:, 1]
                 logger.debug(f"{Fore.GREEN}Fisher Transform calculated.{Style.RESET_ALL}")
             else: logger.warning(f"{Fore.YELLOW}Fisher Transform calculation returned unexpected result. Skipping.{Style.RESET_ALL}")
        else: logger.warning(f"{Fore.YELLOW}Base RSI column '{rsi_col_name}' not found for Fisher Transform.{Style.RESET_ALL}")

        # --- Calculate Swing S/R ---
        swing_window = getattr(config, 'swing_sr_window', 10)
        df_indexed = _calculate_swing_sr(df_indexed, window=swing_window)

        # --- Calculate Order Blocks ---
        atr_col_name = f'ATRr_{atr_period}' # pandas_ta default name for raw ATR
        if atr_col_name not in df_indexed.columns:
            # Fallback if only smoothed ATRt is present (less ideal for OBs)
            atr_col_name = f'ATRt_{atr_period}'
            if atr_col_name not in df_indexed.columns:
                 logger.error(f"{Fore.RED}ATR column not found after pandas_ta calculation. Cannot calculate Order Blocks.{Style.RESET_ALL}")
                 # Add empty OB columns to prevent errors later
                 df_indexed['Is_Bullish_OB'] = False
                 df_indexed['Is_Bearish_OB'] = False
                 df_indexed['OB_High'] = np.nan
                 df_indexed['OB_Low'] = np.nan
                 df_indexed['OB_Type'] = np.nan
            else:
                 logger.warning(f"{Fore.YELLOW}Using smoothed ATR '{atr_col_name}' for Order Blocks. Raw ATR 'ATRr_{atr_period}' preferred.{Style.RESET_ALL}")
                 ob_atr_thresh = getattr(config, 'ob_atr_threshold', 1.5)
                 ob_wick_f = getattr(config, 'ob_wick_factor', None)
                 df_indexed = _calculate_order_blocks(df_indexed, atr_col=atr_col_name, atr_threshold=ob_atr_thresh, wick_factor=ob_wick_f)
        else:
            ob_atr_thresh = getattr(config, 'ob_atr_threshold', 1.5)
            ob_wick_f = getattr(config, 'ob_wick_factor', None)
            df_indexed = _calculate_order_blocks(df_indexed, atr_col=atr_col_name, atr_threshold=ob_atr_thresh, wick_factor=ob_wick_f)


        # --- Calculate and Merge Daily Pivot Points ---
        if daily_df is not None and not daily_df.empty:
            logger.debug(f"{Fore.CYAN}# Calculating and merging Daily Pivot Points...{Style.RESET_ALL}")
            try:
                if not isinstance(daily_df.index, pd.DatetimeIndex):
                     try: daily_df.index = pd.to_datetime(daily_df.index)
                     except Exception as date_err: raise ValueError(f"Failed to convert daily_df index: {date_err}")
                daily_pivots = _calculate_daily_pivot_points(daily_df)
                if not daily_pivots.empty:
                    df_indexed['date'] = df_indexed.index.normalize()
                    df_indexed = pd.merge(df_indexed, daily_pivots, left_on='date', right_index=True, how='left')
                    pivot_cols = daily_pivots.columns.tolist()
                    df_indexed[pivot_cols] = df_indexed[pivot_cols].ffill()
                    df_indexed.drop(columns=['date'], inplace=True)
                    logger.debug(f"{Fore.GREEN}Daily Pivot Points merged.{Style.RESET_ALL}")
                else: logger.warning(f"{Fore.YELLOW}Pivot point calculation resulted empty. Skipping merge.{Style.RESET_ALL}")
            except Exception as pivot_err: logger.error(f"{Fore.RED}Error with pivot points: {pivot_err}{Style.RESET_ALL}", exc_info=True)
        else: logger.debug(f"{Fore.YELLOW}No daily data provided, skipping pivot points.{Style.RESET_ALL}")


        # --- Rename Columns ---
        logger.debug(f"{Fore.CYAN}# Renaming indicator columns...{Style.RESET_ALL}")
        rename_map = {
            f'HMA_{hma_period}': 'HMA',
            f'ZLMA_{zema_period}': 'ZEMA',
            f'SUPERT_{config.supertrend_period}_{config.supertrend_multiplier}': 'Supertrend',
            f'SUPERTd_{config.supertrend_period}_{config.supertrend_multiplier}': 'Supertrend_Direction',
            f'MACD_{config.macd_fast}_{config.macd_slow}_{config.macd_signal}': 'MACD_Line',
            f'MACDh_{config.macd_fast}_{config.macd_slow}_{config.macd_signal}': 'MACD_Hist',
            f'MACDs_{config.macd_fast}_{config.macd_slow}_{config.macd_signal}': 'MACD_Signal',
            f'MOM_{momentum_period}': 'Momentum',
            f'STOCHRSIk_{config.stochrsi_length}_{config.stochrsi_rsi_length}_{config.stochrsi_k}_{config.stochrsi_d}': 'StochRSI_K',
            f'STOCHRSId_{config.stochrsi_length}_{config.stochrsi_rsi_length}_{config.stochrsi_k}_{config.stochrsi_d}': 'StochRSI_D',
            f'ADX_{config.adx_period}': 'ADX',
            f'DMP_{config.adx_period}': 'ADX_DMP',
            f'DMN_{config.adx_period}': 'ADX_DMN',
            f'VOLUME_SMA_{config.volume_sma_period}': 'Volume_SMA',
            f'RSI_{rsi_period}': 'RSI',
            atr_col_name: 'ATR' # Rename the used ATR column to 'ATR'
        }
        df_indexed.rename(columns=rename_map, inplace=True, errors='ignore')


        # --- Post-processing / Combined Signal Flags ---
        logger.debug(f"{Fore.CYAN}# Weaving combined signal flags...{Style.RESET_ALL}")
        # SMA Trend
        sma_short_col, sma_long_col = f'SMA_{config.sma_short}', f'SMA_{config.sma_long}'
        if sma_short_col in df_indexed.columns and sma_long_col in df_indexed.columns:
            df_indexed['SMA_Trend_Up'] = df_indexed[sma_short_col] > df_indexed[sma_long_col]
            df_indexed['SMA_Trend_Down'] = df_indexed[sma_short_col] < df_indexed[sma_long_col]
        else: df_indexed['SMA_Trend_Up'], df_indexed['SMA_Trend_Down'] = False, False

        # Supertrend Direction Flags
        if 'Supertrend_Direction' in df_indexed.columns:
            df_indexed['Supertrend_Up'] = df_indexed['Supertrend_Direction'] == 1
            df_indexed['Supertrend_Down'] = df_indexed['Supertrend_Direction'] == -1
        else: df_indexed['Supertrend_Up'], df_indexed['Supertrend_Down'] = False, False

        # MACD Crossover Flags
        if 'MACD_Line' in df_indexed.columns and 'MACD_Signal' in df_indexed.columns:
            macd_line = pd.to_numeric(df_indexed['MACD_Line'], errors='coerce')
            macd_signal = pd.to_numeric(df_indexed['MACD_Signal'], errors='coerce')
            df_indexed['MACD_Cross_Up'] = (macd_line > macd_signal) & (macd_line.shift(1) <= macd_signal.shift(1))
            df_indexed['MACD_Cross_Down'] = (macd_line < macd_signal) & (macd_line.shift(1) >= macd_signal.shift(1))
            df_indexed['MACD_Cross_Up'] = df_indexed['MACD_Cross_Up'].fillna(False)
            df_indexed['MACD_Cross_Down'] = df_indexed['MACD_Cross_Down'].fillna(False)
        else: df_indexed['MACD_Cross_Up'], df_indexed['MACD_Cross_Down'] = False, False

        # Fisher RSI Crossover Flags
        if 'Fisher_RSI' in df_indexed.columns and 'Fisher_RSI_Signal' in df_indexed.columns:
            fisher_rsi = pd.to_numeric(df_indexed['Fisher_RSI'], errors='coerce')
            fisher_signal = pd.to_numeric(df_indexed['Fisher_RSI_Signal'], errors='coerce')
            df_indexed['Fisher_Cross_Up'] = (fisher_rsi > fisher_signal) & (fisher_rsi.shift(1) <= fisher_signal.shift(1))
            df_indexed['Fisher_Cross_Down'] = (fisher_rsi < fisher_signal) & (fisher_rsi.shift(1) >= fisher_signal.shift(1))
            df_indexed['Fisher_Cross_Up'] = df_indexed['Fisher_Cross_Up'].fillna(False)
            df_indexed['Fisher_Cross_Down'] = df_indexed['Fisher_Cross_Down'].fillna(False)
        else: df_indexed['Fisher_Cross_Up'], df_indexed['Fisher_Cross_Down'] = False, False

        # Restore timestamp from index to column
        df_final = df_indexed.reset_index()

    except Exception as e:
        logger.exception(f"{Fore.RED + Style.BRIGHT}Error calculating indicators: {e}{Style.RESET_ALL}")
        return pd.DataFrame()

    logger.debug(f"{Fore.GREEN}Indicators & structure conjured successfully. Final DataFrame shape: {df_final.shape}{Style.RESET_ALL}")
    return df_final


def update_indicators(df_new_row: pd.DataFrame, config: AppConfig, prev_df_with_indicators: Optional[pd.DataFrame], daily_df: Optional[pd.DataFrame] = None) -> Optional[pd.DataFrame]:
    """
    Efficiently updates indicators and structure by recalculating on the necessary trailing slice.

    Args:
        df_new_row: DataFrame with new OHLCV row(s).
        config: AppConfig spellbook object.
        prev_df_with_indicators: Previous DataFrame containing OHLCV AND calculated indicators/structure.
        daily_df: Optional DataFrame with Daily OHLCV data for pivots.

    Returns:
        DataFrame containing the updated row(s) with indicators/structure, or None if update fails.
    """
    logger.debug(f"{Fore.MAGENTA}Attempting to update indicators & structure for {len(df_new_row)} new row(s). Daily pivots: {'Yes' if daily_df is not None else 'No'}{Style.RESET_ALL}")
    if df_new_row.empty:
        logger.warning(f"{Fore.YELLOW}Received empty DataFrame for update.{Style.RESET_ALL}")
        return None

    required_columns = {"timestamp", "open", "high", "low", "close", "volume"}
    if not required_columns.issubset(df_new_row.columns):
        logger.error(f"{Fore.RED}New row DataFrame missing required columns. Need: {required_columns}. Got: {list(df_new_row.columns)}{Style.RESET_ALL}")
        return None

    # Ensure timestamp column is datetime
    try:
        if not pd.api.types.is_datetime64_any_dtype(df_new_row['timestamp']):
            logger.debug(f"{Fore.CYAN}# New row timestamp requires temporal alignment... converting.{Style.RESET_ALL}")
            df_new_row = df_new_row.copy()
            df_new_row['timestamp'] = pd.to_datetime(df_new_row['timestamp'])
    except Exception as e:
        logger.error(f"{Fore.RED}Failed to convert new row timestamp: {e}{Style.RESET_ALL}", exc_info=True)
        return None

    # --- Determine Required Lookback ---
    # Include window for Swing S/R and buffer for OB look-ahead/ATR calc
    try:
        rsi_period = getattr(config, 'rsi_period', 14)
        fisher_rsi_period = getattr(config, 'fisher_rsi_period', 9)
        momentum_period = getattr(config, 'momentum_period', 10)
        hma_period = getattr(config, 'hma_period', 9)
        zema_period = getattr(config, 'zema_period', 14)
        atr_period = getattr(config, 'atr_period', 14)
        swing_window = getattr(config, 'swing_sr_window', 10)

        # Lookback needs: Max of indicator periods + swing window + buffer
        max_lookback = max(
            config.evt_length * 2,
            atr_period + 1, # ATR needs period + 1 for OB lookahead
            config.sma_long,
            hma_period,
            zema_period * 2,
            config.supertrend_period * 2,
            config.macd_slow + config.macd_signal,
            rsi_period + fisher_rsi_period,
            momentum_period,
            config.stochrsi_length + config.stochrsi_rsi_length + config.stochrsi_d,
            config.adx_period * 2,
            config.volume_sma_period,
            swing_window * 2 + 1, # Swing S/R needs full window on both sides
            100 # Generic buffer
        )
        logger.debug(f"{Fore.CYAN}# Determined maximum lookback requirement: {max_lookback} periods.{Style.RESET_ALL}")
    except AttributeError as e:
         logger.error(f"{Fore.RED}Missing parameter in AppConfig for lookback: {e}. Using default.{Style.RESET_ALL}")
         max_lookback = 400 # Increased fallback


    if prev_df_with_indicators is None or len(prev_df_with_indicators) < max_lookback:
        logger.warning(f"{Fore.YELLOW}Insufficient previous data ({len(prev_df_with_indicators) if prev_df_with_indicators is not None else 0} rows) "
                       f"for lookback ({max_lookback}). Recalculating fully on combined data.{Style.RESET_ALL}")
        combined_df = pd.concat([prev_df_with_indicators, df_new_row], ignore_index=True) if prev_df_with_indicators is not None else df_new_row
        # Drop duplicates just in case, keeping newest
        combined_df.drop_duplicates(subset=['timestamp'], keep='last', inplace=True)
        updated_df = calculate_indicators(combined_df.copy(), config, daily_df)

        if updated_df is None or updated_df.empty:
             logger.error(f"{Fore.RED}Full recalculation failed during update attempt.{Style.RESET_ALL}")
             return None
        new_timestamps = df_new_row['timestamp'].tolist()
        final_updated_rows = updated_df[updated_df['timestamp'].isin(new_timestamps)]
        return final_updated_rows if not final_updated_rows.empty else None

    # --- Combine and Recalculate ---
    try:
        # Take the necessary slice of the past + buffer
        historical_slice = prev_df_with_indicators.tail(max_lookback + 10)

        # Concatenate historical slice with the new row(s) using only core columns
        ohlcv_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
        valid_hist_cols = [col for col in ohlcv_cols if col in historical_slice.columns]
        valid_new_cols = [col for col in ohlcv_cols if col in df_new_row.columns]
        if set(valid_hist_cols) != set(ohlcv_cols) or set(valid_new_cols) != set(ohlcv_cols):
             logger.error(f"{Fore.RED}Core OHLCV columns missing in data slices during update.{Style.RESET_ALL}")
             return None

        combined_df_raw = pd.concat([historical_slice[ohlcv_cols], df_new_row[ohlcv_cols]], ignore_index=True)
        combined_df_raw = combined_df_raw.drop_duplicates(subset=['timestamp'], keep='last').sort_values('timestamp')

        # Recalculate indicators & structure on this combined raw slice
        logger.debug(f"{Fore.CYAN}# Recalculating indicators & structure on combined slice of {len(combined_df_raw)} rows for update.{Style.RESET_ALL}")
        updated_slice_with_indicators = calculate_indicators(combined_df_raw, config, daily_df)

        if updated_slice_with_indicators is None or updated_slice_with_indicators.empty:
            logger.error(f"{Fore.RED}Indicator/structure calculation failed during update on combined slice.{Style.RESET_ALL}")
            return None

        # --- Extract the Updated Row(s) ---
        new_timestamps = df_new_row['timestamp'].tolist()
        if 'timestamp' not in updated_slice_with_indicators.columns:
            logger.error(f"{Fore.RED}'timestamp' column missing in result during update.{Style.RESET_ALL}")
            return None

        final_updated_rows = updated_slice_with_indicators[updated_slice_with_indicators['timestamp'].isin(new_timestamps)]

        if final_updated_rows.empty:
             logger.error(f"{Fore.RED}Could not find updated data for timestamp(s) {new_timestamps} after recalculation.{Style.RESET_ALL}")
             logger.debug(f"Tail of recalculated slice:\n{updated_slice_with_indicators.tail().to_string()}")
             return None

        logger.debug(f"{Fore.GREEN}Successfully updated indicators & structure for {len(final_updated_rows)} new row(s).{Style.RESET_ALL}")
        return final_updated_rows

    except Exception as e:
        logger.exception(f"{Fore.RED + Style.BRIGHT}Unexpected error during indicator update process: {e}{Style.RESET_ALL}")
        return None


# --- Self-Test Snippet (Optional) - A Test of Power & Structure ---
if __name__ == "__main__":
    print(f"{Fore.YELLOW + Style.BRIGHT}--- Running Indicators Module Self-Test (v3.4 - Pyrmethus Weave: Structure) ---{Style.RESET_ALL}")
    logging.basicConfig(level=logging.DEBUG, format=f'{Fore.BLUE}%(asctime)s{Style.RESET_ALL} - {Fore.CYAN}%(name)s{Style.RESET_ALL} - {Fore.MAGENTA}%(levelname)s{Style.RESET_ALL} - %(message)s')

    # Create dummy config for testing
    class TestConfig(AppConfig):
        rsi_period = 14; fisher_rsi_period = 9; momentum_period = 10
        hma_period = 9; zema_period = 14; atr_period = 14
        swing_sr_window = 5 # Smaller window for easier testing
        ob_atr_threshold = 1.0 # Lower threshold for more frequent OBs in test
        ob_wick_factor = 0.6 # Example wick filter

    test_config = TestConfig()
    print(f"\n{Fore.CYAN + Style.BRIGHT}Using Test Config Spellbook:{Style.RESET_ALL}")
    try: print(Fore.WHITE + test_config.model_dump_json(indent=2))
    except AttributeError:
        import json
        attrs = {k: v for k, v in test_config.__dict__.items() if not k.startswith('_')}
        print(Fore.WHITE + json.dumps(attrs, indent=2))

    # Create sample DataFrame
    num_periods = 400 # Ensure enough for lookback
    start_time = pd.Timestamp.now(tz='UTC') - pd.Timedelta(days=4)
    timestamps = pd.date_range(start=start_time, periods=num_periods, freq='5min', name='timestamp')
    print(f"\n{Fore.CYAN}Generating {num_periods} periods of 5min sample data...{Style.RESET_ALL}")
    # Generate data with some swings and potential OB setups
    price = 100 + np.cumsum(np.random.randn(num_periods) * 0.2)
    price[50:60] -= 2 # Down move
    price[100:110] += 2.5 # Up move
    price[150:160] -= 3 # Stronger down move
    price[200:210] += 3.5 # Stronger up move
    noise = np.random.randn(num_periods) * 0.1
    open_p = price + noise
    close_p = price + noise + np.random.randn(num_periods) * 0.05
    high_p = np.maximum(open_p, close_p) + np.random.rand(num_periods) * 0.3
    low_p = np.minimum(open_p, close_p) - np.random.rand(num_periods) * 0.3
    volume_p = np.random.randint(500, 3000, size=num_periods)
    data = {'open': open_p, 'high': high_p, 'low': low_p, 'close': close_p, 'volume': volume_p}
    sample_df = pd.DataFrame(data, index=timestamps)
    sample_df.reset_index(inplace=True)

    # Create dummy Daily data
    print(f"{Fore.CYAN}Generating corresponding daily sample data...{Style.RESET_ALL}")
    sample_df['date'] = sample_df['timestamp'].dt.normalize()
    daily_agg = sample_df.groupby('date').agg(high=('high', 'max'), low=('low', 'min'), close=('close', 'last'))
    sample_daily_df = daily_agg.ffill().bfill()
    sample_df.drop(columns=['date'], inplace=True)

    print(f"\n{Fore.BLUE + Style.BRIGHT}Sample Input 5m DataFrame (first 3 rows):{Style.RESET_ALL}\n{Fore.WHITE}{sample_df.head(3).to_string()}{Style.RESET_ALL}")
    print(f"\n{Fore.BLUE + Style.BRIGHT}Sample Input Daily DataFrame (first 3 rows):{Style.RESET_ALL}\n{Fore.WHITE}{sample_daily_df.head(3).to_string()}{Style.RESET_ALL}")

    # --- Test calculate_indicators ---
    print(f"\n{Fore.MAGENTA + Style.BRIGHT}Testing calculate_indicators with Structure...{Style.RESET_ALL}")
    df_calculated = calculate_indicators(sample_df.copy(), test_config, sample_daily_df.copy())

    if df_calculated is not None and not df_calculated.empty:
        print(f"{Fore.GREEN + Style.BRIGHT}calculate_indicators successful.{Style.RESET_ALL}")
        # Check if new structure columns exist
        structure_cols = ['Resistance', 'Support', 'Is_Swing_High', 'Is_Swing_Low',
                          'Is_Bullish_OB', 'Is_Bearish_OB', 'OB_High', 'OB_Low', 'OB_Type', 'ATR']
        missing_cols = [col for col in structure_cols if col not in df_calculated.columns]
        if not missing_cols:
            print(f"{Fore.GREEN}All key structure columns found.{Style.RESET_ALL}")
        else:
            print(f"{Fore.RED}Missing structure columns: {missing_cols}{Style.RESET_ALL}")
            assert False, f"Missing columns: {missing_cols}"

        # Check some values (allow NaNs initially due to lookback/forward fill)
        assert df_calculated['Resistance'].iloc[50:].notna().any(), "Resistance levels not populated."
        assert df_calculated['Support'].iloc[50:].notna().any(), "Support levels not populated."
        assert df_calculated['Is_Bullish_OB'].any() or df_calculated['Is_Bearish_OB'].any(), "No Order Blocks identified in test data (check generation/threshold)."
        print(f"{Fore.GREEN}Basic checks on structure columns passed.{Style.RESET_ALL}")

        # Display OBs found
        obs_found = df_calculated[df_calculated['Is_Bullish_OB'] | df_calculated['Is_Bearish_OB']]
        print(f"{Fore.CYAN}Identified Order Blocks ({len(obs_found)}):{Style.RESET_ALL}\n{Fore.WHITE}{obs_found[['timestamp', 'OB_Type', 'OB_High', 'OB_Low']].tail().to_string()}{Style.RESET_ALL}")
        print(f"{Fore.BLUE + Style.BRIGHT}Output DataFrame with Indicators & Structure (last 5 rows):{Style.RESET_ALL}\n{Fore.WHITE}{df_calculated.tail().to_string()}{Style.RESET_ALL}")

        # --- Test update_indicators ---
        print(f"\n{Fore.MAGENTA + Style.BRIGHT}Testing update_indicators with Structure...{Style.RESET_ALL}")
        last_row_data = df_calculated.iloc[-1]
        new_timestamp = last_row_data['timestamp'] + pd.Timedelta(minutes=5)
        new_row = pd.DataFrame([{
            'timestamp': new_timestamp, 'open': last_row_data['close'],
            'high': last_row_data['close'] + 0.5, 'low': last_row_data['close'] - 0.2,
            'close': last_row_data['close'] + 0.3, 'volume': 2100
        }])
        print(f"{Fore.BLUE + Style.BRIGHT}New Row DataFrame:{Style.RESET_ALL}\n{Fore.WHITE}{new_row.to_string()}{Style.RESET_ALL}")

        updated_row_df = update_indicators(new_row.copy(), test_config, df_calculated.copy(), sample_daily_df.copy())

        if updated_row_df is not None and not updated_row_df.empty:
            print(f"{Fore.GREEN + Style.BRIGHT}update_indicators successful.{Style.RESET_ALL}")
            assert len(updated_row_df) == 1, "Update should return exactly one row."
            # Check key structure values in the updated row are populated (allow NaN for OBs if none formed)
            assert pd.notna(updated_row_df.iloc[0]['Resistance']), "Resistance is NaN in updated row."
            assert pd.notna(updated_row_df.iloc[0]['Support']), "Support is NaN in updated row."
            assert pd.notna(updated_row_df.iloc[0]['ATR']), "ATR is NaN in updated row."
            print(f"{Fore.GREEN}Key structure indicators in updated row are populated.{Style.RESET_ALL}")

            print(f"{Fore.BLUE + Style.BRIGHT}Updated Row with Indicators & Structure:{Style.RESET_ALL}\n{Fore.WHITE}{updated_row_df.to_string()}{Style.RESET_ALL}")
        else:
            print(f"{Fore.RED + Style.BRIGHT}update_indicators failed.{Style.RESET_ALL}")
            assert False, "update_indicators returned None or empty DataFrame."

    else:
        print(f"{Fore.RED + Style.BRIGHT}calculate_indicators failed.{Style.RESET_ALL}")
        assert False, "calculate_indicators returned None or empty DataFrame."

    print(f"\n{Fore.YELLOW + Style.BRIGHT}--- Indicators Module Self-Test Complete (v3.4) ---{Style.RESET_ALL}")

```

**Key Enhancements:**

1.  **`AppConfig` Parameters**: Added `swing_sr_window`, `ob_atr_threshold`, and `ob_wick_factor` to control the new structure calculations (and included them in the dummy config).
2.  **`_calculate_swing_sr` Function**:
    *   Uses `rolling().max()` and `rolling().min()` over a centered window (`window*2+1`) to efficiently find local highs and lows.
    *   Marks `Is_Swing_High`/`Is_Swing_Low` boolean flags.
    *   Creates `Resistance`/`Support` columns by forward-filling the price levels of the identified swing points.
3.  **`_calculate_order_blocks` Function**:
    *   Requires the ATR column name (`atr_col`).
    *   Identifies potential **Bullish OBs** (last down-candle before a strong up-move breaking the OB's high) and **Bearish OBs** (last up-candle before a strong down-move breaking the OB's low).
    *   "Strong move" is defined relative to the ATR (`ob_atr_threshold`).
    *   Includes an **optional wick filter** (`ob_wick_factor`) to discard OB candles with excessively large wicks relative to their body size.
    *   Adds `Is_Bullish_OB`, `Is_Bearish_OB`, `OB_High`, `OB_Low`, and `OB_Type` columns. **Note**: This marks the *potential OB candle itself*. Your strategy logic will need to track these ranges (`OB_High`, `OB_Low`) and determine if/when they are mitigated (price returns to them).
4.  **Integration into `calculate_indicators`**:
    *   Calls `_calculate_swing_sr` after the main `pandas_ta` calculations.
    *   Calls `_calculate_order_blocks`, ensuring the ATR column (preferably raw `ATRr_X`) is available first. Includes fallback logic and warnings if only smoothed ATR is found.
5.  **Integration into `update_indicators`**:
    *   The `max_lookback` calculation now includes the `swing_sr_window` and buffer for OB calculation.
    *   The recalculation on the combined slice correctly computes the new structural elements for the updated row(s).
6.  **Self-Test (`if __name__ == "__main__":`)**:
    *   Updated the `TestConfig` with structure parameters.
    *   Modified sample data generation to potentially create more swing points and OB setups.
    *   Added assertions to check for the existence and basic validity of the new structure columns (`Resistance`, `Support`, `Is_Bullish_OB`, etc.).
    *   Prints identified OBs during the test run.

This scroll now possesses deeper knowledge of market structure. Remember that Order Block and S/R identification can be complex and subjective; these implementations provide a solid foundation based on common definitions. Use these signals wisely in your trading algorithms!